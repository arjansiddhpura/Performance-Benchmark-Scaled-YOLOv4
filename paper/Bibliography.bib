@misc{hooker2020hardwarelottery,
  title         = {The Hardware Lottery},
  author        = {Sara Hooker},
  year          = {2020},
  eprint        = {2009.06489},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CY},
  url           = {https://arxiv.org/abs/2009.06489}
}

@misc{wang2021scaledyolov4scalingcrossstage,
  title         = {Scaled-YOLOv4: Scaling Cross Stage Partial Network},
  author        = {Chien-Yao Wang and Alexey Bochkovskiy and Hong-Yuan Mark Liao},
  year          = {2021},
  eprint        = {2011.08036},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2011.08036}
}

@misc{wongkinyiu_scaledyolov4_2025,
  author       = {WongKinYiu},
  title        = {Scaled-YOLOv4: Scaling Cross-Stage Partial Network (GitHub Repository)},
  year         = {2025},
  howpublished = {\url{https://github.com/WongKinYiu/ScaledYOLOv4}},
  note         = {GitHub repository; accessed 2025-08-23}
}

@misc{nvidia_a30_datasheet_2022,
  author       = {{NVIDIA Corporation}},
  title        = {NVIDIA A30 Tensor Core GPU Data Sheet},
  year         = {2022},
  month        = mar,
  howpublished = {\url{https://www.nvidia.com/content/dam/en-zz/Solutions/data-center/products/a30-gpu/pdf/a30-datasheet.pdf}},
  note         = {Data sheet; published March 2022; accessed 2025-08-22}
}

@misc{jia2019dissectinggraphcoreipuarchitecture,
  title         = {Dissecting the Graphcore IPU Architecture via Microbenchmarking},
  author        = {Zhe Jia and Blake Tillman and Marco Maggioni and Daniele Paolo Scarpazza},
  year          = {2019},
  eprint        = {1912.03413},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/abs/1912.03413}
}

@article{electronics13112011,
  article-number = {2011},
  author         = {Gepner, Pawe{\l} and Kocot, Bart{\l}omiej and Paprzycki, Marcin and Ganzha, Maria and Moroz, Leonid and Olas, Tomasz},
  doi            = {10.3390/electronics13112011},
  issn           = {2079-9292},
  journal        = {Electronics},
  number         = {11},
  title          = {Performance Evaluation of Parallel Graphs Algorithms Utilizing Graphcore IPU},
  url            = {https://www.mdpi.com/2079-9292/13/11/2011},
  volume         = {13},
  year           = {2024},
  bdsk-url-2     = {https://doi.org/10.3390/electronics13112011}
}

@inproceedings{9567075,
  author    = {Knowles, Simon},
  booktitle = {2021 IEEE Hot Chips 33 Symposium (HCS)},
  title     = {Graphcore},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {1-25},
  keywords  = {Computational modeling;Parallel processing;Data models;Artificial intelligence},
  doi       = {10.1109/HCS52781.2021.9567075}
}

@misc{shekofteh2023performanceanalysisgraphcoreipus,
  title         = {On Performance Analysis of Graphcore IPUs: Analyzing Squared and Skewed Matrix Multiplication},
  author        = {S. -Kazem Shekofteh and Christian Alles and Nils Kochendörfer and Holger Fröning},
  year          = {2023},
  eprint        = {2310.00256},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC},
  url           = {https://arxiv.org/abs/2310.00256}
}

@misc{gholami2024aimemorywall,
  title         = {AI and Memory Wall},
  author        = {Amir Gholami and Zhewei Yao and Sehoon Kim and Coleman Hooper and Michael W. Mahoney and Kurt Keutzer},
  year          = {2024},
  eprint        = {2403.14123},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2403.14123}
}

@article{Valiant1990ABM,
  title   = {A bridging model for parallel computation},
  author  = {Leslie G. Valiant},
  journal = {Commun. ACM},
  year    = {1990},
  volume  = {33},
  pages   = {103-111},
  url     = {https://api.semanticscholar.org/CorpusID:15655597}
}

@misc{graphcore_poplar_2025,
  author       = {{Graphcore}},
  title        = {Poplar{®} Software},
  year         = {2025},
  howpublished = {\url{https://www.graphcore.ai/products/poplar}},
  note         = {Graphcore product page; accessed 2025-11-01}
}

@misc{graphcore_performance_results_2023,
  author       = {{Graphcore}},
  title        = {Performance Results},
  year         = {2023},
  howpublished = {\url{https://www.graphcore.ai/performance-results}},
  note         = {Web page; last updated July 4, 2023; accessed 2025-11-01}
}

@misc{bohl_graphcore_ipus_atpesc2022,
  author       = {Richard Bohl},
  title        = {Graphcore IPUs: Accelerating Argonne’s ML/AI Applications},
  year         = {2022},
  month        = aug,
  howpublished = {\url{https://extremecomputingtraining.anl.gov/wp-content/uploads/sites/96/2022/11/ATPESC-2022-Track-1-Talk-7-Bohl-Graphcore.pdf}},
  note         = {Presented at ATPESC 2022, Track 1 (Hardware Architectures); accessed 2025-11-01}
}

@misc{wang2019cspnetnewbackboneenhance,
  title         = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN},
  author        = {Chien-Yao Wang and Hong-Yuan Mark Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and Jun-Wei Hsieh},
  year          = {2019},
  eprint        = {1911.11929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1911.11929}
}

@misc{liu2018pathaggregationnetworkinstance,
  title         = {Path Aggregation Network for Instance Segmentation},
  author        = {Shu Liu and Lu Qi and Haifang Qin and Jianping Shi and Jiaya Jia},
  year          = {2018},
  eprint        = {1803.01534},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1803.01534}
}

@misc{graphcore_yolov4_pytorch_example_2025,
  author       = {{Graphcore}},
  title        = {examples: vision/yolo\_v4/pytorch (YOLOv4 PyTorch example for Graphcore IPUs)},
  year         = {2025},
  howpublished = {\url{https://github.com/graphcore/examples/tree/master/vision/yolo_v4/pytorch}},
  note         = {GitHub repository directory; accessed 2025-11-01}
}

@misc{graphcore_yolov4_p5_reference_weights_2025,
  author       = {{Graphcore}},
  title        = {YOLOv4 P5 Reference Weights},
  year         = {2025},
  howpublished = {\url{https://gc-demo-resources.s3.us-west-1.amazonaws.com/yolov4_p5_reference_weights.tar.gz}},
  note         = {Archive of YOLOv4 P5 model weights; accessed 2025-11-01}
}


@inproceedings{kazemPMBS,
  author    = {Shekofteh, S.-Kazem and Alles, Christian and Fr\"{o}ning, Holger},
  title     = {Reducing Memory Requirements for the IPU using Butterfly Factorizations},
  year      = {2023},
  isbn      = {9798400707858},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3624062.3624196},
  doi       = {10.1145/3624062.3624196},
  abstract  = {High Performance Computing (HPC) benefits from different improvements during last decades, specially in terms of hardware platforms to provide more processing power while maintaining the power consumption at a reasonable level. The Intelligence Processing Unit (IPU) is a new type of massively parallel processor, designed to speedup parallel computations with huge number of processing cores and on-chip memory components connected with high-speed fabrics. IPUs mainly target machine learning applications, however, due to the architectural differences between GPUs and IPUs, especially significantly less memory capacity on an IPU, methods for reducing model size by sparsification have to be considered. Butterfly factorizations are well-known replacements for fully-connected and convolutional layers. In this paper, we examine how butterfly structures can be implemented on an IPU and study their behavior and performance compared to a GPU. Experimental results indicate that these methods can provide 98.5\% compression ratio to decrease the immense need for memory, the IPU implementation can benefit from 1.3x and 1.6x performance improvement for butterfly and pixelated butterfly, respectively. We also reach to 1.62x training time speedup on a real-word dataset such as CIFAR10.},
  booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
  pages     = {1255–1263},
  numpages  = {9},
  keywords  = {Intelligence Processing Units, Massively Parallel Processing, Sparse Data Structure},
  location  = {Denver, CO, USA},
  series    = {SC-W '23}
}


@article{kazemPARCO,
  title   = {Butterfly Factorization for Vision Transformers on Multi-IPU Systems},
  journal = {Parallel Computing},
  volume  = {VX},
  pages   = {XXX},
  year    = {2025},
  issn    = {0167-8191},
  author  = {S.-Kazem Shekofteh and Daniel Bogacz and Christian Alles and Holger Fröning}
}

@techreport{Graphcore2020Benchmarks,
  author      = {{Graphcore Limited}},
  title       = {Benchmarks},
  institution = {Graphcore Limited},
  year        = {2020},
  month       = {Oct},
  note        = {PDF available at \url{https://www.graphcore.ai/hubfs/assets/pdf/GC_Benchmarks_Oct_20.pdf?hsLang=en}},
  url         = {https://www.graphcore.ai/hubfs/assets/pdf/GC_Benchmarks_Oct_20.pdf?hsLang=en}
}

@misc{he2015deepresiduallearningimage,
  title         = {Deep Residual Learning for Image Recognition},
  author        = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  year          = {2015},
  eprint        = {1512.03385},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1512.03385}
}

@misc{tan2020efficientnetrethinkingmodelscaling,
  title         = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author        = {Mingxing Tan and Quoc V. Le},
  year          = {2020},
  eprint        = {1905.11946},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1905.11946}
}

@misc{xie2017aggregatedresidualtransformationsdeep,
  title         = {Aggregated Residual Transformations for Deep Neural Networks},
  author        = {Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  year          = {2017},
  eprint        = {1611.05431},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1611.05431}
}

@misc{arcelin2021comparisongraphcoreipusnvidia,
  title         = {Comparison of Graphcore IPUs and Nvidia GPUsfor cosmology applications},
  author        = {Bastien Arcelin},
  year          = {2021},
  eprint        = {2106.02465},
  archiveprefix = {arXiv},
  primaryclass  = {physics.comp-ph},
  url           = {https://arxiv.org/abs/2106.02465}
}

@inproceedings{10.1145/3491204.3527469,
  author    = {Sumeet, Nupur and Rawat, Karan and Nambiar, Manoj},
  title     = {Performance Evaluation of GraphCore IPU-M2000 Accelerator for Text Detection Application},
  year      = {2022},
  isbn      = {9781450391597},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3491204.3527469},
  doi       = {10.1145/3491204.3527469},
  booktitle = {Companion of the 2022 ACM/SPEC International Conference on Performance Engineering},
  pages     = {145–152},
  numpages  = {8},
  keywords  = {high-throughput deployment, new technologies, performance evaluation, text detection},
  location  = {Bejing, China},
  series    = {ICPE '22}
}

@misc{lin2015microsoftcococommonobjects,
  title         = {Microsoft COCO: Common Objects in Context},
  author        = {Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
  year          = {2015},
  eprint        = {1405.0312},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1405.0312}
}